{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "597efc57",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A Walk in the `AI` Park"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6ebb46",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A quick appreciation of the AI domain\n",
    "- Gain an understanding of some key concepts\n",
    "- Understand the progression leading upto GenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30b2c1a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# `AI > ML > DL > GenAI`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700979c0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085159c2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Learning Patterns\n",
    "- Supervised Learning: input + expected output\n",
    "    - Classification: is mail spam?, will it rain tomorrow, object detection\n",
    "    - Regression: temperature tomorrow, rain (mm) in monsoon, prob of hitting oil\n",
    "- Unsupervised Learning: only input (!)\n",
    "    - Clustering: you might also like../often bought together../\n",
    "- Others: semi-supervised, self-supervised, human in the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f86bc16",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Requires: scikit-learn 1.7.2, torch 2.6+, numpy 2.3.3\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Generate a toy \"moons\" dataset (nonlinear for illustration)\n",
    "X, y = make_moons(n_samples=1000, noise=0.2)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# 1. Classical ML: Logistic Regression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "print(f\"LogisticRegression test accuracy: {clf.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e159b52c",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.scatterplot(x=X[:,0], y=X[:,1], hue=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5862ebdc",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(x=X[:,0], y=X[:,1], hue=clf.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d3001b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbed8cb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Uses Artificial Neural Networks\n",
    "- Fundamental Unit: Perceptron\n",
    "\n",
    "    > [Input] -> [Weights & Biases] -> [Output]\n",
    "\n",
    "- Arranged in Layers\n",
    "- Ability to automatically adjust W&B : learning\n",
    "- Highly scalable\n",
    "- Terms & concepts: Learning Rate, epochs, Gradient Descent, loss(objective) function, optimizer, batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9567337",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 2. Minimal Neural Net (PyTorch)\n",
    "class TinyNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(2, 16), nn.ReLU(),\n",
    "            nn.Linear(16, 2)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d446efa5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "model = TinyNN()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n",
    "\n",
    "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_t = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_t = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "for epoch in range(160):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(X_train_t)\n",
    "    loss = loss_fn(out, y_train_t)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(f\"NeuralNet test accuracy: {(model(X_test_t).argmax(1) == y_test_t).float().mean().item():.3f}\")\n",
    "\n",
    "# Both models predict classes, but the neural network can fit more complex, nonlinear boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ecc6eb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Others\n",
    "\n",
    "- CNN\n",
    "- RNN\n",
    "- LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515a91f5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857573d9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Evolved NN architectures\n",
    "- Encoder+Decoder\n",
    "- Postional Encoding\n",
    "- **ATTENTION!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08115cbd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Toy transformer encoder & decoder block (PyTorch)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MiniEncoderBlock(nn.Module):\n",
    "    def __init__(self, d_model=8):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, num_heads=2, batch_first=True)\n",
    "        self.ffn = nn.Sequential(nn.Linear(d_model, d_model), nn.ReLU(), nn.Linear(d_model, d_model))\n",
    "    def forward(self, x):\n",
    "        attn_out, _ = self.self_attn(x, x, x)\n",
    "        return self.ffn(attn_out)\n",
    "\n",
    "class MiniDecoderBlock(nn.Module):\n",
    "    def __init__(self, d_model=8):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, 2, batch_first=True)\n",
    "        self.cross_attn = nn.MultiheadAttention(d_model, 2, batch_first=True)\n",
    "        self.ffn = nn.Sequential(nn.Linear(d_model, d_model), nn.ReLU(), nn.Linear(d_model, d_model))\n",
    "    def forward(self, x, enc_out):\n",
    "        x_attn, _ = self.self_attn(x, x, x)\n",
    "        x_ca, _ = self.cross_attn(x_attn, enc_out, enc_out)\n",
    "        return self.ffn(x_ca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f0f979",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "dummy = torch.rand(1, 4, 8)  # (batch, seq_len, d_model)\n",
    "enc = MiniEncoderBlock()\n",
    "dec = MiniDecoderBlock()\n",
    "encoded = enc(dummy)\n",
    "decoded = dec(dummy, encoded)\n",
    "print(\"Encoder output:\", encoded.shape, \"Decoder output:\", decoded.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1e7a33",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "tr = nn.Transformer(\n",
    "    d_model=512,\n",
    "    num_encoder_layers=5,\n",
    "    num_decoder_layers=5,\n",
    "    nhead = 4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaf2a70",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b050742",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b420bd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Toy vocabulary and embeddings\n",
    "main_token = 'orange'\n",
    "tokens = [\"orange\", \"is\", \"my\", \"favourite\", \"fruit\"]\n",
    "\n",
    "embed_dim = 4  # Small, illustrative dimension\n",
    "torch.manual_seed(38)\n",
    "embeddings = torch.randn(len(tokens), embed_dim)\n",
    "\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79779133",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Simplest self-attention (\"orange\" attends to every word)\n",
    "def simple_attention(Q, K, V):\n",
    "    attn_scores = (Q @ K.T) / (K.shape[-1] ** 0.5)\n",
    "    attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "    out = attn_weights @ V\n",
    "    return attn_weights, out\n",
    "\n",
    "Q = K = V = embeddings\n",
    "attn_weights, attn_out = simple_attention(Q, K, V)\n",
    "\n",
    "print(\"Attention weights for 'orange':\")\n",
    "for idx, token in enumerate(tokens):\n",
    "    print(f\"{token:10s}: {attn_weights[0, idx]:.3f}\")\n",
    "\n",
    "# Shows how much \"orange\" (the first word) pays attention to each token (including itself)."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
